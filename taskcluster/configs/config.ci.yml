####
# Blazing fast run config that's also used by Taskcluster CI
###

experiment:
  name: ci
  src: ru
  trg: en

  teacher-ensemble: 1

  mono-max-sentences-src:
    total: 10000
    per-dataset: 10000
  mono-max-sentences-trg:
    total: 10000
    per-dataset: 10000
  spm-sample-size: 1000
  spm-vocab-size: 1000

  best-model: chrf

  use-opuscleaner: "true"
  opuscleaner-mode: "custom"
  teacher-mode: "two-stage"

  bicleaner:
    default-threshold: 0.5
    dataset-thresholds:
      opus_ada83/v1: 0.0
      opus_ELRC-3075-wikipedia_health/v1: 0.6

marian-args:
  training-backward:
    # Run training for 10 updates, and display 5 updates. Only validate and save the
    # model once.
    disp-freq: "2"
    save-freq: "10"
    valid-freq: "10"
    after: 10u
    # Drastically shrink parameters that affect model size:
    dim-vocabs: "128 128"
    enc-depth: "2"
    dec-depth: "2"
    dim-emb: "64"
    dim-rnn: "64"
    transformer-dim-ffn: "256"
    transformer-dim-aan: "256"
    transformer-heads: "2"
    beam-size: "2"
    precision: float16
  training-teacher:
    disp-freq: "2"
    save-freq: "10"
    valid-freq: "10"
    after: 10u
    task: transformer-base
    # Drastically shrink parameters that affect model size:
    dim-vocabs: "128 128"
    enc-depth: "2"
    dec-depth: "2"
    dim-emb: "64"
    dim-rnn: "64"
    transformer-dim-ffn: "256"
    transformer-dim-aan: "256"
    transformer-heads: "2"
    beam-size: "2"
    precision: float16
  training-student:
    disp-freq: "2"
    save-freq: "10"
    valid-freq: "10"
    after: 10u
    # Drastically shrink parameters that affect model size:
    enc-depth: "2"
    dec-depth: "2"
    dim-emb: "64"
    dim-rnn: "64"
    transformer-dim-ffn: "256"
    transformer-dim-aan: "256"
    transformer-heads: "2"
    precision: float16
  training-student-finetuned:
    disp-freq: "2"
    save-freq: "10"
    valid-freq: "10"
    after: 10u
    # Drastically shrink parameters that affect model size:
    enc-depth: "2"
    dec-depth: "2"
    dim-emb: "64"
    dim-rnn: "64"
    transformer-dim-ffn: "256"
    transformer-dim-aan: "256"
    transformer-heads: "2"
    precision: float16
  decoding-backward:
    mini-batch-words: "2000"
  decoding-teacher:
    mini-batch-words: "1000"
    precision: float16

# Ensure that we have adequate coverage for dataset types in CI.
datasets:
  train:
    - opus_ada83/v1
    - opus_ELRC-3075-wikipedia_health/v1
    - url_https://storage.googleapis.com/releng-translations-dev/data/en-ru/pytest-dataset.[LANG].zst
  devtest:
    - flores_dev
    - sacrebleu_aug-upper_wmt19
  test:
    - flores_devtest
  mono-src:
    - news-crawl_news.2008
  mono-trg:
    - news-crawl_news.2007

# Publishes to the "ci" project.
wandb-publication: true
target-stage: all
taskcluster:
  split-chunks: 2
  worker-classes:
    default: gcp-spot
