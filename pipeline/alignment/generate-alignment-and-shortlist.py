#!/usr/bin/env python3

import argparse
import subprocess
import os
import multiprocessing
import sys

def run_command(command):
    print("Running command:")
    print(command)
    try:
        print("Before run")
        result = subprocess.run(command, shell=True, check=True)
        print(f"After run {result.returncode}")
        if result.returncode != 0:
            print("Error running command.")
            print(result.stderr)
            sys.exit(completed_process.returncode)
    except subprocess.CalledProcessError as e:
        print(f"Exception")
        exit(f"Error running command: {e}")

def parse_args():
    parser = argparse.ArgumentParser(description='Generate the alignment and shortlist')

    # Required arguments:
    parser.add_argument('--marian', type=str, required=True,
                        help='The path to the marian build directory')
    parser.add_argument('--source', type=str, required=True,
                        help='The source langage, e.g. "en" or "fr"')
    parser.add_argument('--target', type=str, required=True,
                        help='The target langage, e.g. "en" or "fr"')
    parser.add_argument('--bin', type=str, required=True,
                        help='The path to the binary tools')
    parser.add_argument('--corpus_prefix', type=str, required=True)
    parser.add_argument('--vocab', type=str, required=True,
                        help="The path to the vocab file required by spm_encode. It is generated by spm_train.")
    parser.add_argument('--output_dir', type=str, required=True)

    # Optional arguments.
    parser.add_argument('--threads', type=int, default=multiprocessing.cpu_count(),
                        help='The number of threads to use (default: %(default)s)')
    parser.add_argument('--compression_cmd', type=str, default="pigz",
                        help='The compression command to use (default: %(default)s)')
    parser.add_argument('--artifact_ext', type=str, default="gz",
                        help='The extension to use for the compressed artifact (default: %(default)s)')

    return parser.parse_args()


def tokenize_corpus(args, language: str):
    corpus_path=f"{args.corpus_prefix}.{language}.{args.artifact_ext}"
    output_path=f"{args.output_dir}/corpus.spm.{language}.{args.artifact_ext}"

    print(f"\n\n### Tokenizing a corpus via SentencePiece")
    print(f"==============================================================")
    print(f"This step takes a corpus of text, and converts it into tokens. These tokens")
    print(f"represent sub-word segments. The subwords are defined by the vocab file")
    print(f"generated by spm_train.")
    print(f"")
    print(f" > https://github.com/google/sentencepiece")
    print(f" > Language: {language}")
    print(f" > Corpus: {corpus_path}")
    print(f" > Output: {output_path}\n")

    if os.path.exists(output_path):
        print("Output file already exists, skipping the tokenization.")
        print(output_path)
        return output_path

    if not os.path.exists(corpus_path):
        exit(f"Could not find the corpus: {corpus_path}")


    run_command((
        # Decompress the corpus file and stream it into stdout.
        f"{args.compression_cmd} --decompress --stdout {corpus_path} | "

        # Spread stdout to multiple jobs.
        f"parallel "
        f"  --pipe " # Pipe stdin to the jobs
        f"  --block 50M " # Split stdout into 50M chunks.
        f"  --jobs {args.threads} " # Maximize the jobs for the threads available.
        f"  --no-notice "
        f"  --keep-order " # Maintains the order of stdin -> stdout.

        # Run SentencePiece
        # https://github.com/google/sentencepiece/
        f'"{args.marian}/spm_encode" --model "{args.vocab}" | '

        # Finally compress the results.
        f"{args.compression_cmd} >{args.output_dir}/corpus.spm.{language}.{args.artifact_ext}"
    ))

    print(f'Completed subword segmentation for "{language}"')
    return output_path

def exit(reason: str):
    print(reason)
    print("Exiting with a failure.")
    sys.exit(1)

def main():
    args = parse_args()

    if not os.path.exists(args.output_dir):
        exit(f"The output directory didn't exist: {args.output_dir}")

    if not os.path.exists(args.vocab):
        exit(f"The vocab file did not exist. {args.vocab}")

    tokenize_corpus(args, args.source)
    tokenize_corpus(args, args.target)

if __name__ == "__main__":
    main()
