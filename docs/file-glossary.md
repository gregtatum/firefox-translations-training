---
layout: default
title: Development
nav_order: 10
---

# File Glossary

This file defines the various files that we have, and explains what they are.


### `corpus.spm.{LANG}`

The [SentencePiece](https://github.com/google/sentencepiece) file for a language, which determines the tokenization used. This is typically `unigram` splitting.

## Evaluations

### `{EVAL_DATASET}.metrics.json`

A metrics JSON file that contains the evaluation metrics for a model. It contains the BLEU, chrF, and COMET scores for a model. The various models in the training pipeline each get evaluated with this consistent procedure. This data is reported to Weights and Biases.

### `{EVAL_DATASET}.{SRC_LANG}`
### `{EVAL_DATASET}.{TRG_LANG}`
### `{EVAL_DATASET}.{TRG_LANG}.ref`

Evaluations require 3 files, which are newline separated parallel sentences. The source and target sentences from the original evaluation dataset are copied over as `{EVAL_DATASET}.{SRC_LANG}` and `{EVAL_DATASET}.{TRG_LANG}.ref`. The src sentences are translated by the model and stored as `{EVAL_DATASET}.{TRG_LANG}`.

## Training Artifacts

These artifacts are all generated by:

 * `train-backwards`
 * `train-teacher`
 * `train-student`
 * `finetune-student`

### `devset.out`

The devset is used during training to track the evaluation metrics through the epochs of training. This file is the final devset translated into the target language. It uses augmented data from OpusTrainer.

### `final.model.npz.best-{METRIC}.npz`

The "npz" file that contains the final model. The serialization format is the same as what is produced by [`numpy.savez()`](https://numpy.org/doc/stable/reference/generated/numpy.savez.html). The model with the best metric (typically chrf) is retained as the final.

### `model.npz`

The final model that was produced. This may not be the "best" one.

### `model.npz.best-{METRIC}.npz`

Each metric's best file is retained.

### `model.npz.optimizer.npz`

The state file for the optimizer. This is needed for continuing training. For instance an adam optimizer will contain data for `exp_smoothing`, `adam_mt`, `adam_vt`, `adam_denoms`, `master_parameters`.

### `config.opustrainer.yml`

The YAML config used to configure [OpusTrainer](https://github.com/hplt-project/OpusTrainer) and its training rules. The different models use different strategies for training which can be found in [pipeline/train/configs/opustrainer/student.yml](https://github.com/mozilla/firefox-translations-training/tree/main/pipeline/train/configs/opustrainer).

### `config.opustrainer.yml.state`

??

### `model.npz.yml`

The Marian config.

### `model.{...}.npz.decoder.yml`
### `model.npz.decoder.yml`

Contains the config that defines the model file location, vocab locations, and decoding parameters. Every model npz file has one associated with it.

### `model.npz.progress.yml`

Contains the information about the progress of the training run, for instance how many batches have been processed, how many sentences were sampled, and the best metrics.

### `vocab.spm`

The [SentencePiece](https://github.com/google/sentencepiece) file that is used for tokenization. The same tokenization is applied to source and target sentences.
